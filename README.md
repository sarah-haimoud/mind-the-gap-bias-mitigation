# mind-the-gap-bias-mitigation


# Mind the Gap – Bias Mitigation in Lightweight Transformers

This project explores the detection and mitigation of **social biases** (gender, profession, race, religion) in small language models using the [StereoSet dataset](https://github.com/moinnadeem/stereoset).

We implement and compare multiple fine-tuning strategies to reduce stereotypical completions generated by the model.


## Repository Structure


```
.
├── 01_dataset_exploration/         # Initial exploration of the StereoSet JSON dataset
├── 02_data_preparation/            # Preprocessing and data augmentation steps
├── 03_finetuning/
│   ├── baseline/                   # Standard fine-tuning without mitigation
│   ├── augmented/                  # Fine-tuning with augmented antistereotypical data
│   ├── balanced_augmented/         # Balanced dataset (1:1 stereotype vs antistereotype)
│   └── contrastive/                # Contrastive learning using positive/negative pairs
├── 04_evaluation/                  # Evaluation of biases after each training strategy
├── results/                        # outputs
├── other/                          # Additional or unused notebooks
├── requirements.txt
└── README.md
```

## Methods

We tested the following **bias mitigation strategies**:

| Strategy              | Description                                              |
| --------------------- | -------------------------------------------------------- |
| **Baseline**          | Vanilla fine-tuning without mitigation                   |
| **Augmented**         | Added antistereotypical versions of biased prompts       |
| **Balanced**          | Ensured equal numbers of stereotype/antistereotype pairs |
| **Contrastive**       | Trained on triplets (anchor, positive, negative)         |
| **Adapters** (failed) | Attempted parameter-efficient tuning (did not work)      |


## Evaluation

We used both:

* **Intrasentence bias** 
* **Intersentence bias** 

We provide:

* Accuracy & preference scores (based on StereoSet format)
* Visualizations of **worst stereotypical completions**
* Separate CSV files per evaluation type in `results/`

---

## Outputs

Visualisations with graphs
CSV files such as:

* `pires_cases_intrasentence_augmented.csv`
* `pires_cases_intersentence_contrastive.csv`
* `pires_cases_stereotypes_post.csv`

contain the worst predictions post-mitigation for qualitative inspection.


## Notes

* The models were based on `distilbert-base-uncased`
* All experiments were run in notebooks (see `03_finetuning/`)
* Some evaluations used hand-curated examples for deeper analysis

---

## Author

Sarah Haimoud – Erasmus student from France
