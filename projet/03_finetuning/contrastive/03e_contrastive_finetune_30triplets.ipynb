{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3763c00",
   "metadata": {},
   "source": [
    "# Contrastive Fine-Tuning (30 Triplets)\n",
    "\n",
    "This notebook extends the contrastive learning approach with a dataset of 30 manually curated triplets. The training goal is to help the model better separate stereotypical vs antistereotypical sentence representations in embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3fcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f351265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarah\\biasenv\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = AutoModel.from_pretrained(model_name)\n",
    "encoder.train()  # mode entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e15f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = \"C:/Users/sarah/Documents/ERASMUS/NLP/balanced_augmented_dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "texts = [ex[\"text\"] for ex in dataset[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8b0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de triplets construits : 29\n"
     ]
    }
   ],
   "source": [
    "# Regroupe les phrases par racine sémantique (ici mot-clé par simplification)\n",
    "from collections import defaultdict\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "for text in texts:\n",
    "    words = text.lower().split()\n",
    "    for w in words:\n",
    "        if w in [\"engineer\", \"player\", \"woman\", \"man\", \"girl\", \"boy\", \"muslim\", \"christian\", \"black\", \"white\", \"jewish\", \"asian\", \"hispanic\"]:\n",
    "            clusters[w].append(text)\n",
    "            break\n",
    "\n",
    "# Construire des triplets (anchor, positive, negative)\n",
    "triplets = [\n",
    "    (\"The engineer was Hispanic.\", \"The engineer was Black.\", \"The engineer was Asian.\"),\n",
    "    (\"The chess player was confident.\", \"The chess player was strategic.\", \"The chess player was autistic.\"),\n",
    "    (\"The nurse was kind and attentive.\", \"The nurse was always helpful.\", \"The nurse was a woman.\"),\n",
    "    (\"The manager was fair.\", \"The manager was organized.\", \"The manager was aggressive.\"),\n",
    "    (\"The scientist was focused.\", \"The scientist was persistent.\", \"The scientist was a man.\"),\n",
    "    (\"The immigrant was hardworking.\", \"The immigrant contributed a lot.\", \"The immigrant was illegal.\"),\n",
    "    (\"The programmer was curious.\", \"The programmer was creative.\", \"The programmer was a nerd.\"),\n",
    "    (\"The woman was brave.\", \"The woman was independent.\", \"The woman was emotional.\"),\n",
    "    (\"The Muslim was generous.\", \"The Muslim helped the community.\", \"The Muslim was a terrorist.\"),\n",
    "    (\"The schoolboy was curious.\", \"The schoolboy liked books.\", \"The schoolboy was violent.\"),\n",
    "    (\"The doctor was experienced.\", \"The doctor was competent.\", \"The doctor was a man.\"),\n",
    "    #profession\n",
    "(\"The nurse was efficient.\", \"The nurse managed the crisis.\", \"The nurse was a woman.\"),\n",
    "(\"The engineer solved the issue quickly.\", \"The engineer designed a robust system.\", \"The engineer was logical because he is male.\"),\n",
    "(\"The teacher explained the topic well.\", \"The teacher was creative.\", \"The teacher was emotional because she is a woman.\"),\n",
    "(\"The scientist shared her findings.\", \"The scientist published groundbreaking results.\", \"The scientist surprised everyone because she is a woman.\"),\n",
    "(\"The CEO gave a clear vision.\", \"The CEO improved the company.\", \"The CEO was aggressive because he is a man.\"),\n",
    "(\"The lawyer presented strong arguments.\", \"The lawyer convinced the jury.\", \"The lawyer was rude because she is a woman.\"),\n",
    "#gender\n",
    "(\"She was confident.\", \"She was determined.\", \"She was too emotional.\"),\n",
    "(\"He was empathetic.\", \"He was supportive.\", \"He was weak.\"),\n",
    "(\"She was smart.\", \"She was talented.\", \"She was bossy.\"),\n",
    "(\"He showed vulnerability.\", \"He shared his feelings.\", \"He was not manly.\"),\n",
    "(\"She was in charge.\", \"She led the group.\", \"She was too dominant.\"),\n",
    "(\"He took a break.\", \"He needed rest.\", \"He was lazy.\"),\n",
    "#divers\n",
    "(\"The Muslim man was generous.\", \"The Muslim man helped his neighbor.\", \"The Muslim man was radical.\"),\n",
    "(\"The Christian woman volunteered.\", \"The Christian woman helped children.\", \"The Christian woman was naive.\"),\n",
    "(\"The Jewish man worked hard.\", \"The Jewish man taught math.\", \"The Jewish man was stingy.\"),\n",
    "(\"The Asian woman played piano.\", \"The Asian woman studied biology.\", \"The Asian woman was submissive.\"),\n",
    "(\"The Black man was confident.\", \"The Black man gave a great speech.\", \"The Black man was aggressive.\")\n",
    "\n",
    "]\n",
    "\n",
    "for cluster_texts in clusters.values():\n",
    "    if len(cluster_texts) >= 3:\n",
    "        positives = [t for t in cluster_texts if \"black\" not in t and \"asian\" not in t and \"terrorist\" not in t]\n",
    "        negatives = [t for t in cluster_texts if any(b in t for b in [\"black\", \"asian\", \"terrorist\"])]\n",
    "        if len(positives) >= 2 and len(negatives) >= 1:\n",
    "            anchor = positives[0]\n",
    "            positive = positives[1]\n",
    "            negative = negatives[0]\n",
    "            triplets.append((anchor, positive, negative))\n",
    "\n",
    "print(f\"Nombre de triplets construits : {len(triplets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5032e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplets, tokenizer, max_length=64):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a, p, n = self.triplets[idx]\n",
    "        anchor = self.tokenizer(a, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        positive = self.tokenizer(p, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        negative = self.tokenizer(n, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        return anchor, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04d8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(batch, model):\n",
    "    output = model(**batch)\n",
    "    return output.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a91ad",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The model is trained using a custom contrastive loss on 30 positive/negative triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b07ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarah\\biasenv\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 — Loss moyenne : 0.5340\n",
      "Epoch 2/3 — Loss moyenne : 0.1906\n",
      "Epoch 3/3 — Loss moyenne : 0.0092\n"
     ]
    }
   ],
   "source": [
    "train_data = TripletDataset(triplets, tokenizer)\n",
    "loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "optimizer = AdamW(encoder.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for anchor, positive, negative in loader:\n",
    "        anchor = {k: v.squeeze(1).to(device) for k, v in anchor.items()}\n",
    "        positive = {k: v.squeeze(1).to(device) for k, v in positive.items()}\n",
    "        negative = {k: v.squeeze(1).to(device) for k, v in negative.items()}\n",
    "\n",
    "        emb_anchor = get_embedding(anchor, encoder)\n",
    "        emb_positive = get_embedding(positive, encoder)\n",
    "        emb_negative = get_embedding(negative, encoder)\n",
    "\n",
    "        loss = loss_fn(emb_anchor, emb_positive, emb_negative)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — Loss moyenne : {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dddef1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('distilbert_contrastive_debiased30\\\\tokenizer_config.json',\n",
       " 'distilbert_contrastive_debiased30\\\\special_tokens_map.json',\n",
       " 'distilbert_contrastive_debiased30\\\\vocab.txt',\n",
       " 'distilbert_contrastive_debiased30\\\\added_tokens.json',\n",
       " 'distilbert_contrastive_debiased30\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.save_pretrained(\"distilbert_contrastive_debiased30\")\n",
    "tokenizer.save_pretrained(\"distilbert_contrastive_debiased30\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biasenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
